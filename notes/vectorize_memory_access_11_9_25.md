## Why does transposing the shared memory for matrix A (As) help?
When As is in row-major format, accessing a portion of the column we need from As (each thread needs elements from consecutive rows from the same column) requires strided memory accesses (the indexing formula for row-major is row * width + column - assuming a block width and height of 4, accessing row 0, column 0 and row 1 column 0 would require accesses to As[0] and As[4]).   However, when As is in column-major format, accessing the column portion requires no stride because the column values are stored adjacent, allowing for vectorization. Using the same example above and the fact that the indexing formula for column-major is column * height + row, accessing row 0, column 0 and row 1 column 0 would require accesses to As[0 * 4 + 0] and As[0 * 4 + 1] which comes out ot As[0] and As[1] which are adjacent. This particular example shows how vectorizing two memory accesses works, but it extends to any number of vectorized memory accesses (for example, using a float4 to load 4 consecutive floats at once).

## Why do we need to fully parenthesize macro parameters?
Macro parameters need to be fully parameterized to avoid operator precedence issues. For example, the CEIL_DIV macro we use is defined as CEIL_DIV(M, N) (((M) + (N)-1) / (N)). If we instead defined it as CEIL_DIV(M, N) (M + (N - 1)) / N, an operation like 10 * CEIL_DIV(a, 8) would result in subtle errors. 10 * CEIL_DIV(a, 8) with the incorrect definition would expand to 10 * (a + (8 - 1)) / N -> 10 * (a + 7) / N. The multiplication would happen before the division completes, which is incorrect. For the fully parenthesized version, it would expand to 10 * (((a) + (8)-1) / (N)) -> 10 * ((a + 7) / N) which results in the correct output.

## What is the purpose of the template <const int BM, const int BN, const int BK, const int TM, const int TN>?
Using the template instead of runtime parameters allows for the dimensions to be known at compile-time which allows the compiler to make better optimizations. The compiler couldn’t do things like unrolling loops if the various parameters are defined at runtime. The calculations for array indexing formulas become compile-time constants when the template is used, but they would become arithmetic address computation operations during runtime if the template wasn’t used. Using the template allows for shared memory arrays to get optimal allocation and have all memory offsets precomputed. It also allows for register allocation to be more efficient as the compiler knows exactly how many variables each thread needs. If the number of registers needed is unknown, the compiler has to conservatively assume that the data might not fit in the registers and be forced to use local memory instead of registers.

## Initial Indexing for A, B, and C:
We are multiplying an M x N matrix by an N x K matrix to get a M x K matrix. For A, the initial formula is cRow * BM * N - the block row is denoted by cRow and each block has BM rows. In order to skip cRow * BM rows, we have to multiply cRow * BM by N as each row has N elements. For B, we simply multiply cCol * BN to get to the correct column as the block column is denoted by cCol and each block has BN columns. For the initial indexing of C, we combine the two but slightly modify the first term as there are K elements in each row instead of N. Thus, we end up with cRow * BM * K + cCol * BN.

## Indexing for transposing A into shared memory As:
I was originally a little confused by the indexing formula, but I realized that transpose indexing just requires an inverse of regular indexing. The formula for regular indexing is ROW * WIDTH + COLUMN, but for transposed indexing, it is COLUMN * HEIGHT + ROW. The HEIGHT value for blocks in A is BM, and the COLUMN value is given by innerColA * 4 + OFFSET. We multiply innerColA by 4 as each thread is loading four elements, and the OFFSET can be either 0, 1, 2, or 3 depending on which specific element out of the 4 the particular thread is loading. 

## Core Computation:
The outer dotIdx loop iterates through the K dimension - I think of it as taking slices of the rows from A and columns from B. It’s a little counterintuitive because the slices of the rows from A are shaped as columns, while the slices of the columns from B are shaped as rows. The slices from As are loaded with the following formula: dotIdx * BM + threadRow * TM + i. This is using the COLUMN * HEIGHT + ROW formula as As is stored in column-major format. dotIdx is from the outer loop, and it represents which column/slice we are currently processing (remember the slices for A are column-shaped as they extend across rows). dotIdx * BM skips to the column we need (BM is HEIGHT), and threadRow * TM skips to the starting row of the slice threadRow is the y-dimension of the threaded, while TM represents how many elements each thread processes in the y-dimension. The i element iterates from 0 to- TM - 1 and loads all the elements from the relevant slice which contains TM elements. For B’s shared memory, the indexing is dotIdx * BN + threadCol * TN + i which follows similar logic. dotIdx * BN skips to the row which contains the slice we want, and threadCol * TN skips the column processed by other threads while i iterates through each element of the slice to load TN elements.

The nested loop computes the outer product for each pair of slices and adds the output to threadResults. Each element from regM is multiplied with each element from regN (outer product) and the accumulated product is stored in threadResults[resIdxM * TN + resIdxN] where resIdxM is the index of the element from regM and resIdxN is the index of the element from regN. The indexing is storing the results with row-major format in threadResults. The accumulation is needed as similar computation happens across all pairs of slices, and the results need to be accumulated so that threadResults has the sum of all computations after all threads are complete.